{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938db7a-1663-4cba-81e8-539d42c46ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00c44ee-a06d-4390-8695-e5da3f82de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(inputs, se_ratio=0.25):\n",
    "    filters = inputs.shape[-1]\n",
    "    se_filters = max(1, int(filters * se_ratio))\n",
    "    se = layers.GlobalAveragePooling2D()(inputs)\n",
    "    se = layers.Reshape((1, 1, filters))(se)\n",
    "    se = layers.Conv2D(se_filters, 1, activation='relu')(se)\n",
    "    se = layers.Conv2D(filters, 1, activation='sigmoid')(se)\n",
    "    return layers.Multiply()([inputs, se])\n",
    "    \n",
    "\n",
    "def cbam_block(inputs, reduction_ratio=16):\n",
    "    # Channel Attention\n",
    "    channel_avg = layers.GlobalAveragePooling2D()(inputs)\n",
    "    channel_max = layers.GlobalMaxPooling2D()(inputs)\n",
    "    \n",
    "    shared_dense = tf.keras.Sequential([\n",
    "        layers.Dense(inputs.shape[-1] // reduction_ratio, activation='relu'),\n",
    "        layers.Dense(inputs.shape[-1])\n",
    "    ])\n",
    "\n",
    "    avg_out = shared_dense(channel_avg)\n",
    "    max_out = shared_dense(channel_max)\n",
    "\n",
    "    channel_attention = layers.Add()([avg_out, max_out])\n",
    "    channel_attention = layers.Activation('sigmoid')(channel_attention)\n",
    "    channel_attention = layers.Reshape((1, 1, inputs.shape[-1]))(channel_attention)\n",
    "    x = layers.Multiply()([inputs, channel_attention])\n",
    "\n",
    "    # Spatial Attention using only Keras layers\n",
    "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(x)\n",
    "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(x)\n",
    "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "    spatial_attention = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "    x = layers.Multiply()([x, spatial_attention])\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "def mbconv_block(inputs, out_channels, expansion_factor, kernel_size, strides, se_ratio=0.25):\n",
    "    in_channels = inputs.shape[-1]\n",
    "    x = inputs\n",
    "\n",
    "    # Expansion phase\n",
    "    if expansion_factor != 1:\n",
    "        x = layers.Conv2D(in_channels * expansion_factor, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('swish')(x)\n",
    "\n",
    "    # Depthwise conv\n",
    "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    # Squeeze-and-Excitation\n",
    "    x = se_block(x, se_ratio=se_ratio)\n",
    "\n",
    "    # Projection phase\n",
    "    x = layers.Conv2D(out_channels, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Skip connection\n",
    "    if strides == 1 and in_channels == out_channels:\n",
    "        x = layers.Add()([inputs, x])\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "def EfficientNetB0_custom(input_shape=(224,224, 3), num_classes=38, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Stem\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    # MBConv blocks (adapted from official B4 config)\n",
    "    # (repeats, out_channels, kernel_size, strides)\n",
    "    x = mbconv_block(x,out_channels=16,expansion_factor=1,kernel_size=3,strides=1)\n",
    "    block_configs = [\n",
    "        (2, 24, 3, 2),\n",
    "        (2, 40, 5, 2),\n",
    "        (3, 80, 3, 2),\n",
    "        (3, 112, 5, 1),\n",
    "        (4, 192, 5, 2),\n",
    "        (1, 320, 3, 1),\n",
    "    ]\n",
    "\n",
    "    expansion_factor = 6\n",
    "    for repeats, out_channels, kernel_size, strides in block_configs:\n",
    "        for i in range(repeats):\n",
    "            x = mbconv_block(\n",
    "                x,\n",
    "                out_channels=out_channels,\n",
    "                expansion_factor=expansion_factor,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides if i == 0 else 1\n",
    "            )\n",
    "\n",
    "    # Head\n",
    "    x = layers.Conv2D(1280, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    # Convolutional Block Attention Module\n",
    "    x = cbam_block(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
